{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets\n",
        "! pip install datasets[audio]\n",
        "# ! pip install jiwer\n",
        "! pip install torchaudio\n",
        "! pip install Levenshtein\n",
        "! pip install timit_per\n",
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ],
      "metadata": {
        "id": "VciMQO7J5k1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install accelerate>=0.21.0\n",
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ],
      "metadata": {
        "id": "WNQ7anbi60QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl95z0LiH7kd",
        "outputId": "ce9a1bde-314f-4027-b2f5-7667c4287c84"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1vIFxMIfXjdT4ie7c6JXWNpNDGaXKDHiH'\n",
        "output = 'timid_2.'\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "--_HaLdF4stY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip -o /content/timid_2.zip"
      ],
      "metadata": {
        "id": "DI2XPv0uvVP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2K3QBHb5L_K"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"timit_asr\", data_dir=\"/content/data/lisa/data\", trust_remote_code=True)\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPQAwz37IZ1f",
        "outputId": "4c423b5c-6719-4446-c6b2-426f0e4d5cff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'file': Value(dtype='string', id=None),\n",
              " 'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),\n",
              " 'text': Value(dtype='string', id=None),\n",
              " 'phonetic_detail': Sequence(feature={'start': Value(dtype='int64', id=None), 'stop': Value(dtype='int64', id=None), 'utterance': Value(dtype='string', id=None)}, length=-1, id=None),\n",
              " 'word_detail': Sequence(feature={'start': Value(dtype='int64', id=None), 'stop': Value(dtype='int64', id=None), 'utterance': Value(dtype='string', id=None)}, length=-1, id=None),\n",
              " 'dialect_region': Value(dtype='string', id=None),\n",
              " 'sentence_type': Value(dtype='string', id=None),\n",
              " 'speaker_id': Value(dtype='string', id=None),\n",
              " 'id': Value(dtype='string', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoModelForCTC, AutoTokenizer, AutoFeatureExtractor, Wav2Vec2Processor\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(DEVICE)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
        "\n",
        "resample_layer = torchaudio.transforms.Resample(\n",
        "    orig_freq=48000,\n",
        "    new_freq=16000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_jm0GXK8lci",
        "outputId": "eddf5df0-8439-49e5-c6f4-0b3cc5d2b6b3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import jiwer\n",
        "\n",
        "import tqdm\n",
        "\n",
        "def evaluate_model(test_dataset):\n",
        "    model.eval()\n",
        "    total_samples = len(test_dataset)\n",
        "    total_errors = 0\n",
        "    total_words = 0\n",
        "    i=0\n",
        "\n",
        "    with torch.no_grad(), tqdm.tqdm(total=total_samples) as pbar:\n",
        "        for sample in test_dataset:\n",
        "            audio_input, sample_rate = torchaudio.load(sample[\"file\"], normalize=True, channels_first=True, num_frames=100000)  # Adjust num_frames as needed\n",
        "            audio_input = resample_layer(audio_input)\n",
        "            inputs = processor(audio_input.squeeze(0), sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "            input_values = inputs.input_values.to(DEVICE)\n",
        "            attention_mask = torch.ones_like(input_values).to(DEVICE)\n",
        "\n",
        "            logits = model(input_values=input_values,\n",
        "                           attention_mask=attention_mask).logits\n",
        "\n",
        "            pred_ids = torch.argmax(logits, dim=-1)\n",
        "            predicted_transcription = processor.batch_decode(pred_ids)[0]\n",
        "            ground_truth_transcription = sample[\"text\"]\n",
        "\n",
        "            # per\n",
        "            error = jiwer.compute_measures(ground_truth_transcription, predicted_transcription)[\"wer\"]\n",
        "            total_errors += error\n",
        "            total_words += len(ground_truth_transcription.split())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    per = total_errors / total_words\n",
        "    accuracy = 1 - per\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "evaluate_model(test_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ5ViVa8K1xv",
        "outputId": "0c736d32-63f0-49f1-afa3-a4f49f7a150a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1680/1680 [12:25<00:00,  2.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8842258214761314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZmPCyp_ePCs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kGDHhyZRPCxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDVt2e1YPC1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import jiwer\n",
        "import tqdm\n",
        "\n",
        "\n",
        "class LinearHead(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearHead, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "class MLPHead(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MLPHead, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def evaluate_model(test_dataset, layer_name, head_name):\n",
        "    model.eval()\n",
        "    total_samples = len(test_dataset)\n",
        "    total_errors = 0\n",
        "    total_words = 0\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    with torch.no_grad(), tqdm.tqdm(total=total_samples) as pbar:\n",
        "        for sample in test_dataset:\n",
        "            i += 1\n",
        "            if i > 30: break\n",
        "            audio_input, sample_rate = torchaudio.load(sample[\"file\"], normalize=True, channels_first=True, num_frames=100000)\n",
        "            audio_input = resample_layer(audio_input)\n",
        "            inputs = processor(audio_input.squeeze(0), sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "            input_values = inputs.input_values.to(DEVICE)\n",
        "            attention_mask = torch.ones_like(input_values).to(DEVICE)\n",
        "\n",
        "            if layer_name == \"last_hidden_state\":\n",
        "                logits = model(input_values=input_values, attention_mask=attention_mask).logits\n",
        "            elif layer_name in input_values:\n",
        "                hidden_states = model(input_values=input_values, attention_mask=attention_mask).hidden_states\n",
        "                logits = model.from_pretrained(model_name, config=model.config, input_values=hidden_states[layer_name]).logits\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid layer name: {layer_name}\")\n",
        "\n",
        "            if head_name == \"linear\":\n",
        "                head = LinearHead(input_dim=logits.shape[-1], output_dim=len(tokenizer.vocab))\n",
        "            elif head_name == \"mlp\":\n",
        "                hidden_dim = 512\n",
        "                head = MLPHead(input_dim=logits.shape[-1], hidden_dim=hidden_dim, output_dim=len(tokenizer.vocab))\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid head name: {head_name}\")\n",
        "\n",
        "            head_logits = head(logits)\n",
        "\n",
        "            pred_ids = torch.argmax(head_logits, dim=-1)\n",
        "            predicted_transcription = processor.batch_decode(pred_ids)[0]\n",
        "            ground_truth_transcription = sample[\"text\"]\n",
        "\n",
        "            error = jiwer.compute_measures(ground_truth_transcription, predicted_transcription)[\"wer\"]\n",
        "            total_errors += error\n",
        "            total_words += len(ground_truth_transcription.split())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    per = total_errors / total_words\n",
        "    accuracy = 1 - per\n",
        "    print(f\"\\nAccuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "WPVpDcfQK1zx"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, Wav2Vec2Processor, Wav2Vec2ForCTC, HubertForCTC, AutoTokenizer, AutoFeatureExtractor\n",
        "\n",
        "model_name = \"facebook/hubert-large-ls960-ft\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
        "model = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\").to(DEVICE)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "resample_layer = torchaudio.transforms.Resample(\n",
        "    orig_freq=48000,\n",
        "    new_freq=16000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcV3QHxNO5Io",
        "outputId": "90906460-a251-40a1-ff3f-2b03f7411b92"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertForCTC: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertForCTC were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(test_dataset, \"last_hidden_state\", \"linear\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udJc8iUeK113",
        "outputId": "5a80984c-d43d-4d16-fbf9-0d0be9dd0891"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 30/1680 [00:54<49:46,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8845785440613027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(test_dataset, \"last_hidden_state\", \"mlp\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vppfr366K138",
        "outputId": "710e37e4-f0b4-4901-9a72-6e9367b07dc3"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 30/1680 [01:00<55:48,  2.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.8850574712643678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoModelForCTC, AutoTokenizer, AutoFeatureExtractor, Wav2Vec2Processor\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(DEVICE)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
        "\n",
        "resample_layer = torchaudio.transforms.Resample(\n",
        "    orig_freq=48000,\n",
        "    new_freq=16000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HBtT1pLK19s",
        "outputId": "e0392631-4eb3-4790-bb56-337cad3b6ee9"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(test_dataset, \"last_hidden_state\", \"linear\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpRTux94Yggv",
        "outputId": "05de4c11-e031-4ea2-aafe-3b664d9c8a03"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 30/1680 [00:17<16:26,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.8826628352490421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(test_dataset, \"last_hidden_state\", \"mlp\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whCzNCWkK15k",
        "outputId": "605ed827-02d9-4f43-9b10-36ad913139c3"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 30/1680 [00:30<27:48,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.8850574712643678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}